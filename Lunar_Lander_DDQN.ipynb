{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lunar Lander using DDQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import load_model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from collections import deque\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory class for storing and retrieving previous transition details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Storage(object):\n",
    "    def __init__(self, max_size, n_states, n_actions, discrete=False):\n",
    "        self.mem_size = max_size\n",
    "        self.mem_cntr = 0\n",
    "        self.discrete = discrete\n",
    "        self.state_memory = np.zeros((self.mem_size, n_states))\n",
    "        self.new_state_memory = np.zeros((self.mem_size, n_states))\n",
    "        \n",
    "        dtype = np.int8 if self.discrete else np.float32\n",
    "        \n",
    "        self.action_memory = np.zeros((self.mem_size, n_actions), dtype=dtype)\n",
    "        self.reward_memory = np.zeros(self.mem_size)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
    "\n",
    "    def store_transition(self, state, action, reward, state_, done):\n",
    "        index = self.mem_cntr % self.mem_size\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = state_\n",
    "\n",
    "        if self.discrete:\n",
    "            actions = np.zeros(self.action_memory.shape[1])\n",
    "            actions[action] = 1.0\n",
    "            self.action_memory[index] = actions\n",
    "        else:\n",
    "            self.action_memory[index] = action\n",
    "\n",
    "        self.reward_memory[index] = reward\n",
    "        self.terminal_memory[index] = done\n",
    "        self.mem_cntr += 1\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "        batch = np.random.choice(max_mem, batch_size)\n",
    "\n",
    "        states = self.state_memory[batch]\n",
    "        actions = self.action_memory[batch]\n",
    "        rewards = self.reward_memory[batch]\n",
    "        states_ = self.new_state_memory[batch]\n",
    "        terminal = self.terminal_memory[batch]\n",
    "\n",
    "        return states, actions, rewards, states_, terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double Deep Learning Agent Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DDQNAgent(object):\n",
    "    def __init__(self, n_actions, n_states, gamma, batch_size, epsilon = 1.0,\n",
    "                 epsilon_decay = 0.996, epsilon_min = 0.01, mem_size = 1000000,\n",
    "                 lr = 0.01, replace_target = 100, discrete = True):\n",
    "        self.action_space = [i for i in range(n_actions)]\n",
    "        self.state_space = [i for i in range(n_states)]\n",
    "        self.n_actions = n_actions\n",
    "        self.n_states = n_states\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.memory_size = mem_size\n",
    "        self.replace_target = replace_target\n",
    "        self.discrete = discrete\n",
    "        self.memory = Storage(self.memory_size, self.n_states, self.n_actions, self.discrete)\n",
    "        self.lr = lr\n",
    "        self.q_eval = self.build_model()\n",
    "        self.q_target = self.build_model()\n",
    "        self._counter = 0\n",
    "        \n",
    "    def remember(self, state, action, reward, new_state, done):\n",
    "        self.memory.store_transition(state, action, reward, new_state, done)\n",
    "    \n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(256, input_shape = (self.n_states,), activation = 'relu'))\n",
    "        model.add(Dense(256, activation = 'relu'))\n",
    "        model.add(Dense(self.n_actions))\n",
    "        model.compile(optimizer = Adam(learning_rate = self.lr), loss = 'mse')\n",
    "        return model\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        rand = np.random.random()\n",
    "        state = np.reshape(state, (1,self.n_states))\n",
    "        if rand < self.epsilon:\n",
    "            action = random.choice(self.action_space)\n",
    "        else:\n",
    "            action = np.argmax(self.q_eval.predict(state))\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def learn(self):\n",
    "        if self.memory.mem_cntr <= self.batch_size:\n",
    "            return\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)\n",
    "        \n",
    "        action_values = np.array(self.action_space, dtype=np.int8)\n",
    "        actions = np.dot(actions, action_values)\n",
    "        \n",
    "        dones = 1-dones\n",
    "\n",
    "        next_action_values_by_q_target = self.q_target.predict(next_states)\n",
    "        next_action_values_by_q_eval = self.q_eval.predict(next_states)\n",
    "        pres_action_values_by_q_eval = self.q_eval.predict(states)\n",
    "        \n",
    "        max_actions = np.argmax(next_action_values_by_q_eval, axis = 1)\n",
    "        \n",
    "        q_target = pres_action_values_by_q_eval\n",
    "        \n",
    "        batch_indices = np.arange(self.batch_size, dtype = int)\n",
    "\n",
    "        q_target[batch_indices, actions.astype(int)] = rewards + self.gamma*next_action_values_by_q_target[batch_indices, max_actions.astype(int)]*dones\n",
    "        \n",
    "        _ = self.q_eval.fit(states, q_target, verbose = 0)\n",
    "        \n",
    "    \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon = self.epsilon*self.epsilon_decay\n",
    "            \n",
    "        if self._counter % self.replace_target == 0:\n",
    "            self.update_q_target()\n",
    "                \n",
    "    def update_q_target(self):\n",
    "        self.q_target.set_weights(self.q_eval.get_weights())\n",
    "            \n",
    "    def save_model(self, fname):\n",
    "        self.q_eval.save(fname)\n",
    "\n",
    "    def load_model(self, fname):\n",
    "        self.q_eval = load_model(fname)\n",
    "        if self.epsilon == 0.01:\n",
    "            self.update_network_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making environment and defining state and action size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "for eno in range(10):\n",
    "    env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        observation,reward,done,other_info = env.step(action)\n",
    "        score += reward\n",
    "        if done:\n",
    "            print(\"Episode %d/%d: score : %d\"%(eno+1, 20, score))\n",
    "            break\n",
    "#     print(\"Episode %d/%d: score : %d/%d\"%(eno+1, 20, i, 1000))\n",
    "print('finished')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "agent = DDQNAgent(n_actions = action_size, n_states = state_size, gamma = 0.99, batch_size = 64, epsilon = 1.0,\n",
    "                 epsilon_decay = 0.996, epsilon_min = 0.01, mem_size = 1000000,\n",
    "                 lr = 0.0005, replace_target = 100)\n",
    "n_episodes = 4000\n",
    "scores = []\n",
    "avg_scores = []\n",
    "\n",
    "try: \n",
    "    for e in range(n_episodes):\n",
    "        \n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        done = False\n",
    "        t = 0\n",
    "        while not done:\n",
    "            env.render()\n",
    "            \n",
    "            action = agent.choose_action(state)\n",
    "            \n",
    "            next_state, reward, done, other_info = env.step(action)\n",
    "            \n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "            score += reward\n",
    "            \n",
    "            agent.learn()\n",
    "            t+=1\n",
    "        \n",
    "        scores.append(score)\n",
    "        avg_score = np.mean(scores[max(0, len(scores)-100) : len(scores)])\n",
    "        avg_scores.append(avg_score)\n",
    "        \n",
    "        print(\"Episode: {}/{}, score : {:.6}, avg_score : {:.6} Exploration: {:.2}, Time States: {}\".format(e, n_episodes, score, avg_score, agent.epsilon, t))\n",
    "        \n",
    "        if e%50 == 0:\n",
    "            plt.plot(scores[-100:])\n",
    "            plt.show()\n",
    "            plt.plot(avg_scores)\n",
    "            plt.show()\n",
    "            plt.figure(figsize=(30,20))\n",
    "            plt.plot(avg_scores)\n",
    "            plt.title('Avg_Score')\n",
    "            plt.savefig(f'./LunarLander_graphs/graph_{e}.png', quality = 95)\n",
    "            plt.close()\n",
    "            agent.save_model(f\"./LunarLander_weights/q_eval_{e}.h5\")\n",
    "            \n",
    "\n",
    "    print('Model Trained')\n",
    "    env.close()\n",
    "except OSError as err:\n",
    "    print(\"OS error: {0}\".format(err))\n",
    "    env.close()\n",
    "    print(\"Closing Environment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay model with trained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('./LunarLander_weights/q_eval_3650.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/20: score : 273\n",
      "Episode 2/20: score : 263\n",
      "Episode 3/20: score : 244\n",
      "Episode 4/20: score : 240\n",
      "Episode 5/20: score : 232\n",
      "Episode 6/20: score : 247\n",
      "Episode 7/20: score : 285\n",
      "Episode 8/20: score : 252\n",
      "Episode 9/20: score : 259\n",
      "Episode 10/20: score : 279\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "env.reset()\n",
    "for eno in range(10):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    while not done:\n",
    "        env.render()\n",
    "        state = np.reshape(state, (1,8))\n",
    "        action = np.argmax(model.predict(state))\n",
    "        next_state, reward, done, other_info = env.step(action)\n",
    "        state = next_state\n",
    "        score += reward\n",
    "        if done:\n",
    "            print(\"Episode %d/%d: score : %d\"%(eno+1, 20, score))\n",
    "            break\n",
    "#     print(\"Episode %d/%d: score : %d/%d\"%(eno+1, 20, i, 1000))\n",
    "print('finished')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
